{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease     \n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Get:5 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2496 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1301 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2069 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [31.2 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2972 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\n",
      "Get:12 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1006 kB]\n",
      "Get:13 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [1937 kB]\n",
      "Get:14 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [28.5 kB]\n",
      "Fetched 12.3 MB in 4s (2894 kB/s)                            \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libflac-dev libflac8 libglib2.0-0 libglib2.0-data libicu66 libogg-dev\n",
      "  libogg0 libsndfile1 libvorbis-dev libvorbis0a libvorbisenc2 libvorbisfile3\n",
      "  libxml2 pkg-config shared-mime-info tzdata xdg-user-dirs\n",
      "The following NEW packages will be installed:\n",
      "  libflac-dev libflac8 libglib2.0-0 libglib2.0-data libicu66 libogg-dev\n",
      "  libogg0 libsndfile1 libsndfile1-dev libvorbis-dev libvorbis0a libvorbisenc2\n",
      "  libvorbisfile3 libxml2 pkg-config shared-mime-info tzdata xdg-user-dirs\n",
      "0 upgraded, 18 newly installed, 0 to remove and 52 not upgraded.\n",
      "Need to get 12.6 MB of archives.\n",
      "After this operation, 53.3 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libglib2.0-0 amd64 2.64.6-1~ubuntu20.04.4 [1287 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libglib2.0-data all 2.64.6-1~ubuntu20.04.4 [6052 B]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 tzdata all 2022g-0ubuntu0.20.04.1 [286 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libicu66 amd64 66.1-2ubuntu2.1 [8515 kB]\n",
      "13% [4 libicu66 5004 B/8515 kB 0%]"
     ]
    }
   ],
   "source": [
    "# 環境構築\n",
    "\n",
    "!apt-get update\n",
    "!apt-get install -y libsndfile1-dev\n",
    "!pip install scipy\n",
    "!pip install scikit-learn\n",
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clips 作成\n",
    "\n",
    "SAMPLING_RATE = 44100 # 変更不可\n",
    "\n",
    "clip_size = 81920 # 楽曲を再構築するパーツ1つあたりの大きさ\n",
    "step_size = 20480 # clip をずらすときの大きさ\n",
    "window_size = 10240 # CQT_CHROMA を取得するのに使用するサンプル数\n",
    "hop_size = 640 # window をずらすときの大きさ\n",
    "\n",
    "from scipy.io.wavfile import read, write\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import sys\n",
    "\n",
    "if os.path.isfile(\"../data/out/clips.wav\"):\n",
    "    print(\"../data/out/clips.wav exists.\")\n",
    "    files = [\"../data/out/clips.wav\"]\n",
    "else:\n",
    "    # clips.npy をもとにデータを作成する \n",
    "    if os.path.isfile(\"../data/arrays/clips.npy\"):\n",
    "        print(\"loading ../data/arrays/clips.npy ...\")\n",
    "        clips = np.load(\"../data/arrays/clips.npy\")\n",
    "        print(\"creating ../data/out/clips.wav ...\")\n",
    "        write(\"../data/out/clips.wav\", SAMPLING_RATE, clips.reshape((clips.shape[0] * clips.shape[1], )))\n",
    "        files = [\"../data/out/clips.wav\"]\n",
    "        \n",
    "    # clips,npy がないとき /data/wav44100 内の WAV ファイルを参照する\n",
    "    else:\n",
    "        files = glob.glob(\"../data/wav44100/*.wav\")\n",
    "        #files = [\"../data/wav44100/3DEmbodimentFromLines.wav\"] # デバッグ用 \n",
    "        \n",
    "if len(files) == 0:\n",
    "    print(\"WAV ファイルが見つかりませんでした。\")\n",
    "    sys.exit(1)\n",
    "        \n",
    "clips_filename = \"../data/arrays/c\" + str(clip_size) + \"_s\" + str(step_size) + \"_f32_clips\"\n",
    "\n",
    "if os.path.isfile(clips_filename + \".npy\"):\n",
    "    print(\"loading \" + clips_filename + \".npy ...\")\n",
    "    clips = np.load(clips_filename + \".npy\")\n",
    "else:\n",
    "    print(\"creating \" + clips_filename + \".npy ...\")\n",
    "    raw_data_list = [librosa.load(file, sr=SAMPLING_RATE)[0] for file in files] # 左の音だけ使う\n",
    "\n",
    "    clips = np.zeros((0, clip_size), dtype=np.float32)\n",
    "    for raw_data in raw_data_list:\n",
    "        tmp = [raw_data[i:i + clip_size] for i in range(0, len(raw_data) - clip_size, step_size)]\n",
    "        clips = np.vstack((clips, np.array(tmp)))\n",
    "    np.save(clips_filename, clips)\n",
    "\n",
    "print(\"The clip array has \" + str(clips.shape[0]) + \" clips.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cqt 作成\n",
    "\n",
    "n_bins = 84\n",
    "\n",
    "cqt_filename = clips_filename + \"_w\" + str(window_size) + \"_h\" + str(hop_size) + \"_cqts\"\n",
    "\n",
    "if os.path.isfile(cqt_filename + \".npy\"):\n",
    "    print(\"loading \" + cqt_filename + \".npy ...\")\n",
    "    cqts = np.load(cqt_filename + \".npy\")\n",
    "else:\n",
    "    print(\"creating \" + cqt_filename + \".npy ...\")\n",
    "    num = 0\n",
    "    for clip in clips:\n",
    "        print(\"cqt progress: clip \" + str(num + 1) + \" / \" + str(len(clips)))\n",
    "        \n",
    "        tmp_cqt = librosa.cqt(clip, sr=SAMPLING_RATE, hop_length=hop_size, n_bins=n_bins)\n",
    "        tmp_cqt = tmp_cqt.reshape((1, tmp_cqt.shape[0], tmp_cqt.shape[1]))\n",
    "        \n",
    "        if num == 0:\n",
    "            cqts = tmp_cqt\n",
    "        else:\n",
    "            cqts = np.vstack((cqts, tmp_cqt))\n",
    "            \n",
    "        num += 1\n",
    "\n",
    "    np.save(cqt_filename, cqts)\n",
    "    \n",
    "print(\"The cqt array has \" + str(cqts.shape[0]) + \" cqts.\")\n",
    "print(\"cqts.shape: \" + str(cqts.shape))\n",
    "print(\"Type(cqts[0][0][0]): \" + str(type(cqts[0][0][0])))\n",
    "print(\"np.max(cqts[0][0]): \" + str(np.max(cqts[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cqt_choroma 作成\n",
    "\n",
    "def Preprocessing(array):\n",
    "    array = np.abs(array)\n",
    "    array = np.log(array + 1)\n",
    "    array = array / np.log(np.finfo(np.float32).max)\n",
    "    array = array.T\n",
    "    return array\n",
    "\n",
    "cqt_chroma_filename = clips_filename + \"_w\" + str(window_size) + \"_h\" + str(hop_size) + \"_cqt_chromas\"\n",
    "\n",
    "if os.path.isfile(cqt_chroma_filename + \".npy\"):\n",
    "    print(\"loading \" + cqt_chroma_filename + \".npy ...\")\n",
    "    cqt_chromas= np.load(cqt_chroma_filename + \".npy\")\n",
    "else:\n",
    "    print(\"creating \" + cqt_chroma_filename + \".npy ...\")\n",
    "    num = 0\n",
    "    for cqt in cqts:\n",
    "        print(\"cqt_chroma progress: clip \" + str(num + 1) + \" / \" + str(len(cqts)))\n",
    "        \n",
    "        tmp_cqt_chroma = librosa.feature.chroma_cqt(C=cqt, sr=SAMPLING_RATE)\n",
    "        tmp_cqt_chroma = Preprocessing(tmp_cqt_chroma)\n",
    "        tmp_cqt_chroma = tmp_cqt_chroma.reshape((1, tmp_cqt_chroma.shape[0], tmp_cqt_chroma.shape[1]))\n",
    "        \n",
    "        if num == 0:\n",
    "            cqt_chromas = tmp_cqt_chroma\n",
    "        else:\n",
    "            cqt_chromas = np.vstack((cqt_chromas, tmp_cqt_chroma))\n",
    "            \n",
    "        num += 1\n",
    "\n",
    "    np.save(cqt_chroma_filename, cqt_chromas)\n",
    "    \n",
    "print(\"The cqt_chroma array has \" + str(cqt_chromas.shape[0]) + \" cqt_chromas.\")\n",
    "print(\"cqt_chromas.shape: \" + str(cqt_chromas.shape)) # clip 番号、window 番号、 chroma 番号 になる\n",
    "print(\"Type(cqt_chromas[0][0][0]): \" + str(type(cqt_chromas[0][0][0])))\n",
    "print(\"np.max(cqt_chromas[0][0]): \" + str(np.max(cqt_chromas[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_norm 作成\n",
    "\n",
    "num_divide = 8\n",
    "\n",
    "max_norm_filename = clips_filename + \"_d\" + str(num_divide)\n",
    "\n",
    "if os.path.isfile(max_norm_filename + \".npy\"):\n",
    "    print(\"loading \" + max_norm_filename + \".npy ...\")\n",
    "    max_norms= np.load(max_norm_filename + \".npy\")\n",
    "else:\n",
    "    print(\"creating \" + max_norm_filename + \".npy ...\")\n",
    "    num = 0\n",
    "    for clip in clips:\n",
    "        print(\"max_norm progress: clip \" + str(num + 1) + \" / \" + str(len(clips)))\n",
    "        max_norm = [np.max(clip[int((i / num_divide) * len(clip)):int(((i + 1) / num_divide) * len(clip))] ** 2) for i in range(num_divide)]\n",
    "    \n",
    "        if num == 0:\n",
    "            max_norms = np.array(max_norm)\n",
    "        else:\n",
    "            max_norms = np.vstack((max_norms, np.array(max_norm)))\n",
    "        num += 1\n",
    "        \n",
    "    np.save(max_norm_filename, max_norms)\n",
    "    \n",
    "print(\"The max_norm array has \" + str(max_norms.shape[0]) + \" max_norms.\")\n",
    "print(\"max_norms.shape: \" + str(max_norms.shape))\n",
    "print(\"Type(max_norms[0][0]): \" + str(type(max_norms[0][0])))\n",
    "print(\"np.max(max_norms[0]): \" + str(np.max(max_norms[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データとテストデータに分ける\n",
    "\n",
    "def normalize(array):\n",
    "    if (np.sum(array) == 0):\n",
    "        return array\n",
    "    else:\n",
    "        return array / np.sum(array)\n",
    "    \n",
    "cqt_chroma_sum_threshold = 0.01 # 次の window の sum が閾値に満たないときに除外します\n",
    "test_data_rate = 0.2\n",
    "weight = 0.01 # max_norm の重み\n",
    "\n",
    "window_num_per_clip = cqt_chromas.shape[1]\n",
    "\n",
    "cqt_chromas = np.array([np.hstack((cqt_chromas[i], np.repeat(np.array([max_norms[i]]), cqt_chromas.shape[1], axis=0) * weight)) for i in range(len(max_norms))])\n",
    "cqt_chromas = np.concatenate([cqt_chromas[:-1, :, :], cqt_chromas[1:, 0, :].reshape(cqt_chromas.shape[0] - 1, 1, cqt_chromas.shape[2])], 1)\n",
    "\n",
    "clips = np.delete(clips, np.where(np.sum(cqt_chromas[:,-1,:], axis=1) < cqt_chroma_sum_threshold)[0], axis=0)\n",
    "cqt_chromas = np.delete(cqt_chromas, np.where(np.sum(cqt_chromas[:,-1,:], axis=1) < cqt_chroma_sum_threshold)[0], axis=0)\n",
    "\n",
    "p = np.random.permutation(len(cqt_chromas))\n",
    "cqt_chromas = cqt_chromas[p]\n",
    "clips = clips[p]\n",
    "\n",
    "x_test = np.apply_along_axis(normalize, 2, cqt_chromas[:int(cqt_chromas.shape[0] * test_data_rate), :window_num_per_clip, :].reshape(int(cqt_chromas.shape[0] * test_data_rate), window_num_per_clip, cqt_chromas.shape[2], 1))\n",
    "x_train = np.apply_along_axis(normalize, 2, cqt_chromas[int(cqt_chromas.shape[0] * test_data_rate):, :window_num_per_clip, :].reshape(cqt_chromas.shape[0] - int(cqt_chromas.shape[0] * test_data_rate), window_num_per_clip, cqt_chromas.shape[2], 1))\n",
    "y_test =  np.apply_along_axis(normalize, 1, cqt_chromas[:int(cqt_chromas.shape[0] * test_data_rate), window_num_per_clip, :])\n",
    "y_train = np.apply_along_axis(normalize, 1, cqt_chromas[int(cqt_chromas.shape[0] * test_data_rate):, window_num_per_clip, :])\n",
    "\n",
    "print(\"x_train.shape: \" + str(x_train.shape))\n",
    "print(\"x_test.shape: \" + str(x_test.shape))\n",
    "print(\"y_train.shape: \" + str(y_train.shape))\n",
    "print(\"y_test.shape: \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    # 精度の履歴をプロット\n",
    "    plt.plot(history.history['accuracy'],\"o-\",label=\"accuracy\")\n",
    "    plt.plot(history.history['val_accuracy'],\"o-\",label=\"val_acc\")\n",
    "    plt.title('model accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    " \n",
    "    # 損失の履歴をプロット\n",
    "    plt.plot(history.history['loss'],\"o-\",label=\"loss\",)\n",
    "    plt.plot(history.history['val_loss'],\"o-\",label=\"val_loss\")\n",
    "    plt.title('model loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル作成\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, Conv2D, Flatten, Input, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "learning_rate = 0.0001\n",
    " \n",
    "model = keras.models.Sequential()\n",
    "model.add(Conv2D(32, (3, 1), activation=\"relu\", input_shape=(window_num_per_clip, cqt_chromas.shape[2], 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 1)))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Conv2D(64, (3, 1), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 1)))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Conv2D(128, (3, 1), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 1)))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Conv2D(256, (3, 1), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 1)))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=cqt_chromas.shape[2]))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(learning_rate=learning_rate),metrics=['accuracy'])\n",
    "\n",
    "model.build()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習\n",
    "batch_size = 256\n",
    "epochs = 5\n",
    "model_name = \"cqt_chroma_batch\" + str(batch_size) + \"_e\" + str(epochs)\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "model.save(\"../data/models/\" + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss 遷移のプロット\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 楽曲の出力\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from keras.models import load_model\n",
    "import soundfile as sf\n",
    "\n",
    "model = load_model(\"../data/models/\" + model_name)\n",
    "file_name = \"out_\" + model_name\n",
    "num_clips = 10\n",
    "\n",
    "def add_fade(x, fadetime, sr):\n",
    "    ft_len = int(fadetime*sr)\n",
    "    r = np.arange(0, ft_len)*np.pi/ft_len\n",
    "    w_fo = (0.5+0.5*np.cos(r))**0.5\n",
    "    w_fi = (0.5-0.5*np.cos(r))**0.5\n",
    "    \n",
    "    x[0:ft_len]        *= w_fi\n",
    "    x[clip_size-ft_len::] *= w_fo\n",
    "    return x\n",
    "\n",
    "def gen_xfade(x_pre, x_next, fadetime, sr):\n",
    "    ft_len = int(fadetime*sr)\n",
    "    if x_pre is None:\n",
    "        xfade = x_next\n",
    "    else:\n",
    "        x_pre_begin = x_pre[:x_pre.shape[0] - ft_len]\n",
    "        x_pre_end = x_pre[x_pre.shape[0] - ft_len:]\n",
    "        x_pre_len = clip_size\n",
    "        x_next_len = clip_size\n",
    "        x_pre_len -= ft_len\n",
    "        x_next_len -= ft_len\n",
    "        xfade = np.concatenate((x_pre_begin, np.concatenate((x_pre_end, np.zeros(x_next_len))) + x_next))\n",
    "    return xfade\n",
    "\n",
    "def create_music(fname):\n",
    "    first_index = np.random.randint(0, len(x_test))\n",
    "    predict_index = first_index\n",
    "    out = None\n",
    "    \n",
    "    for i in range(num_clips):\n",
    "        print(\"-- generate \" + str(i + 1) + \" / \" + str(num_clips))\n",
    "        predict = model.predict(np.array([x_test[predict_index]]))\n",
    "\n",
    "        index = 0\n",
    "        similar_index = 0\n",
    "        cos_sim = -1\n",
    "        for cqt_chroma in x_test[:, 0:1, :, 0]:\n",
    "            tmp_sim = cosine_similarity(predict, cqt_chroma) \n",
    "            if tmp_sim > cos_sim:\n",
    "                cos_sim = tmp_sim\n",
    "                similar_index = index\n",
    "            index += 1\n",
    "\n",
    "        print(\"cos_sim: \" + str(cos_sim))\n",
    "        print(\"predict: \" + str(predict))\n",
    "        print(\"x_test[similar_index]: \" + str(x_test[similar_index, 0, :, 0]))\n",
    "        print(\"similar_index: \" + str(similar_index))\n",
    "        print(\"--\")\n",
    "\n",
    "        predict_index = similar_index\n",
    "\n",
    "        tmp = add_fade(clips[predict_index], 0.1, SAMPLING_RATE)\n",
    "        out = gen_xfade(out, tmp, 0.1, SAMPLING_RATE)\n",
    "    \n",
    "    \n",
    "    sf.write(fname, out, SAMPLING_RATE, subtype=\"PCM_16\")\n",
    "\n",
    "fname = \"../data/out/\" + file_name + \"_1.wav\"\n",
    "print(\"creating \" + fname + \" ...\")\n",
    "create_music(fname)\n",
    "\n",
    "fname = \"../data/out/\" + file_name + \"_2.wav\"\n",
    "print(\"creating \" + fname + \" ...\")\n",
    "create_music(fname)\n",
    "\n",
    "fname = \"../data/out/\" + file_name + \"_3.wav\"\n",
    "print(\"creating \" + fname + \" ...\")\n",
    "create_music(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
